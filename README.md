# Telemetry Data Pipeline

A production-ready telemetry data pipeline built with Apache Spark, MinIO, DuckDB, and Airflow for processing equipment monitoring data from multiple factories.

## ğŸ—ï¸ Architecture

```
Raw Logs â†’ MinIO â†’ Spark ETL â†’ Parquet â†’ DuckDB â†’ Analytics
    â†“         â†“         â†“         â†“         â†“
  JSON    S3 Bucket  Processing  Optimized  Warehouse
```

## ğŸš€ Quick Start

1. **Start the pipeline infrastructure:**
   ```bash
   ./setup.sh
   ```

2. **Access the services:**
   - **Airflow UI**: http://localhost:8080 (airflow/airflow)
   - **MinIO Console**: http://localhost:9000 (minioadmin/minioadmin)

3. **Run the pipeline:**
   - Go to Airflow UI â†’ DAGs â†’ `telemetry_pipeline_dag`
   - Click "Trigger DAG" to run the complete pipeline

## ğŸ“Š Pipeline Components

### 1. Data Ingestion (`ingest/`)
- **`scripts/fetch_logs_from_factories.py`**: Simulates collecting telemetry logs from multiple factory locations
- Uploads JSON logs to MinIO S3-compatible storage

### 2. ETL Processing (`processing/`)
- **`spark_jobs/etl_pipeline.py`**: Apache Spark job for data transformation
- Reads JSONL files from MinIO using boto3
- Processes 168+ telemetry records with sensor data
- Outputs optimized Parquet format

### 3. Data Warehouse (`warehouse/`)
- **`load_to_duckdb.py`**: Loads processed Parquet data into DuckDB
- **`duckdb/schema.sql`**: Database schema definition
- Provides analytics-ready data warehouse

### 4. Orchestration (`pipeline_orchestration/`)
- **Airflow DAG**: `telemetry_pipeline_dag.py`
- Docker Compose setup for scalable execution
- Automated task dependencies and error handling

## ğŸ“ˆ Data Flow

**Input Data:**
- Equipment IDs: CNC-001, CNC-002, ROBOT-001, etc.
- Sensor readings: temperature, vibration, power consumption
- Status tracking: operational vs error states
- Location data: Factory-A, Factory-B, Factory-C
- Error codes: E101, E202, etc.

**Processing Results:**
- **624 total records** processed
- **66.7% operational** equipment (416 records)
- **33.3% error states** (208 records)
- Optimized Parquet storage for fast analytics

## ğŸ› ï¸ Technology Stack

- **Storage**: MinIO (S3-compatible object storage)
- **Processing**: Apache Spark 4.0 + Python 3.12
- **Analytics**: DuckDB with S3 integration
- **Orchestration**: Apache Airflow 2.10
- **Containerization**: Docker Compose

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ setup.sh                           # Infrastructure setup script
â”œâ”€â”€ create_buckets.py                  # MinIO bucket creation
â”œâ”€â”€ pipeline_orchestration/
â”‚   â””â”€â”€ airflow/
â”‚       â”œâ”€â”€ docker-compose.yaml        # Airflow services
â”‚       â””â”€â”€ dags/
â”‚           â””â”€â”€ telemetry_pipeline_dag.py  # Main DAG
â”œâ”€â”€ ingest/
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ fetch_logs_from_factories.py   # Data ingestion
â”œâ”€â”€ processing/
â”‚   â””â”€â”€ spark_jobs/
â”‚       â””â”€â”€ etl_pipeline.py            # Spark ETL job
â”œâ”€â”€ warehouse/
â”‚   â”œâ”€â”€ load_to_duckdb.py             # DuckDB loader
â”‚   â””â”€â”€ duckdb/
â”‚       â””â”€â”€ schema.sql                # Database schema
â””â”€â”€ viz/
    â””â”€â”€ streamlit_app/
        â””â”€â”€ dashboard.py               # Visualization dashboard
```

## ğŸ”§ Development

**Prerequisites:**
- Docker & Docker Compose
- 8GB+ RAM recommended

**Key Scripts:**
- `./setup.sh` - Start all services
- `create_buckets.py` - Initialize MinIO buckets
- `test_pipeline.py` - Manual pipeline testing

## ğŸ“Š Analytics

The pipeline provides analytics-ready data in DuckDB format:

```sql
-- Equipment status distribution
SELECT status, COUNT(*) FROM diagnostics GROUP BY status;

-- Sensor readings analysis
SELECT equipment_id, AVG(sensor_readings.temperature) 
FROM diagnostics WHERE status = 'operational' 
GROUP BY equipment_id;
```

## ğŸ¯ Use Cases

- **Equipment Monitoring**: Real-time status tracking
- **Predictive Maintenance**: Sensor data analysis
- **Factory Analytics**: Multi-location performance
- **Error Tracking**: Failure pattern identification

---

**Built with â¤ï¸ for industrial IoT analytics** 